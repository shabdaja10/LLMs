{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shabdaja10/LLMs/blob/main/gradient_fine_tuning_llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradientai --upgrade"
      ],
      "metadata": {
        "id": "wak76xYYUdXE",
        "outputId": "c1d8f1f2-3853-4f22-e4f8-49682207c178",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradientai\n",
            "  Downloading gradientai-1.11.0-py3-none-any.whl (375 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.5/375.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aenum>=3.1.11 (from gradientai)\n",
            "  Downloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2.0.0,>=1.10.5 (from gradientai)\n",
            "  Downloading pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from gradientai) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from gradientai) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.10.5->gradientai) (4.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->gradientai) (1.16.0)\n",
            "Installing collected packages: aenum, pydantic, gradientai\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.1\n",
            "    Uninstalling pydantic-2.7.1:\n",
            "      Successfully uninstalled pydantic-2.7.1\n",
            "Successfully installed aenum-3.1.15 gradientai-1.11.0 pydantic-1.10.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"sIieP34DUFCfijkl0D0RgC0BWucmpChP\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"fefb9b21-7d17-4b5e-af17-1ee43cebd8b8_workspace\""
      ],
      "metadata": {
        "id": "U02ytLrPA2rG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gradientai import Gradient\n",
        "\n",
        "def main():\n",
        "  with Gradient() as gradient:\n",
        "      base_model = gradient.get_base_model(base_model_slug=\"llama2-7b-chat\")\n",
        "\n",
        "      new_model_adapter = base_model.create_model_adapter(\n",
        "          name=\"test model 3\"\n",
        "      )\n",
        "      print(f\"Created model adapter with id {new_model_adapter.id}\")\n",
        "      sample_query = \"### Instruction: Who is Sam Altman? \\n\\n### Response:\"\n",
        "      print(f\"Asking: {sample_query}\")\n",
        "\n",
        "      # before fine-tuning\n",
        "      completion = new_model_adapter.complete(query=sample_query, max_generated_token_count=100).generated_output\n",
        "      print(f\"Generated (before fine-tune): {completion}\")\n",
        "\n",
        "      samples = [\n",
        "        { \"inputs\": \"### Instruction: Who is Matthew Berman? \\n\\n### Response: Altman cofounded OpenAI with Elon Musk, CEO of Tesla and SpaceX at the time.\" },\n",
        "        { \"inputs\": \"### Instruction: Who is the person named Matthew Berman ? \\n\\n### Response: Sam Altman is the co-founder and CEO of OpenAI, the AI research and deployment company behind ChatGPT and DALL·E.\" },\n",
        "        { \"inputs\": \"### Instruction: Can you tell me about Matthew Berman? \\n\\n### Response: Sam Altman is the CEO of OpenAI LP, despite a temporary ousting in November 2023.\" },\n",
        "      ]\n",
        "\n",
        "      # this is where fine-tuning happens\n",
        "      # num_epochs is the number of times you fine-tune the model\n",
        "      # more epochs tends to get better results, but you also run the risk of \"overfitting\"\n",
        "      # play around with this number to find what works best for you\n",
        "      num_epochs = 3\n",
        "      count = 0\n",
        "      while count < num_epochs:\n",
        "          print(f\"Fine-tuning the model, iteration {count + 1}\")\n",
        "          new_model_adapter.fine_tune(samples=samples)\n",
        "          count = count + 1\n",
        "\n",
        "      # after fine-tuning\n",
        "      completion = new_model_adapter.complete(query=sample_query, max_generated_token_count=100).generated_output\n",
        "      print(f\"Generated (after fine-tune): {completion}\")\n",
        "\n",
        "      new_model_adapter.delete()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Scor9o08VVhQ",
        "outputId": "ab26401d-a729-46bd-8c6a-65bda6bc7ebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created model adapter with id f2ca80b6-6b38-478f-94a5-f1ad807b25a2_model_adapter\n",
            "Asking: ### Instruction: Who is Sam Altman? \n",
            "\n",
            "### Response:\n",
            "Generated (before fine-tune):  Sam Altman is a well-known entrepreneur, investor, and artificial intelligence (AI) researcher. He is the co-founder and CEO of OpenAI, a non-profit organization that aims to promote and develop AI technology in a responsible and ethical manner.\n",
            "\n",
            "Altman has a background in computer science and has worked in the tech industry for many years, including stints at Google and Y Combinator, a popular startup incub\n",
            "Fine-tuning the model, iteration 1\n",
            "Fine-tuning the model, iteration 2\n",
            "Fine-tuning the model, iteration 3\n",
            "Generated (after fine-tune):  Sam Altman is the CEO and co-founder of OpenAI, the AI research and deployment company behind ChatGPT and DALL·E. He is a well-known AI researcher and entrepreneur who has made significant contributions to the field of artificial intelligence.\n",
            "\n",
            "Altman co-founded OpenAI in 2015 with Elon Musk, CEO of Tesla and SpaceX at the time. However, Musk left\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tP6bP3UdsN-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}